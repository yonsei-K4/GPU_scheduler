import os
import json
import torch
from tqdm import tqdm
import numpy as np
# from profile.runner import ModelRunner
from session import SessionManager
from collections import defaultdict

iter = 1000

if __name__ == "__main__":
    import json
    import os
    import time

    print("ğŸš€ [Standalone] ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡  ì‹œì‘")

    # ëª¨ë¸ ëª©ë¡ ë¡œë“œ
    model_file_path = os.path.join(os.path.dirname(__file__), "models.json")
    try:
        with open(model_file_path, "r") as f:
            model_data = json.load(f)
            model_list = model_data.get("models", [])
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ëª©ë¡ ë¡œë”© ì‹¤íŒ¨: {e}")
        exit(1)

    if not model_list:
        print("âŒ ëª¨ë¸ ëª©ë¡ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        exit(1)

    # Provider ì„¤ì •
    providers = {m: ("CUDAExecutionProvider", {"device_id": 0}) for m in model_list}

    # ëª¨ë¸ ë¡œë”©
    for model_name in model_list:
        session = SessionManager.fetch_model(model_name, providers[model_name], batch_size=1)
        if session is None:
            print(f"âŒ ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨")
            continue
        else:
            print(f"âœ… ëª¨ë¸ {model_name} ë¡œë“œ ì™„ë£Œ")

    # ì¶”ë¡  ì‹¤í–‰
    latency_stats = defaultdict(list)  # ëª¨ë¸ë³„ latency(ms) ì €ì¥


    # ë¯¸ë¦¬ í•œë²ˆ ì‹¤í–‰í•˜ì—¬ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ë†“ê¸°
    for model_name in model_list:
            result = SessionManager.run_model(model_name)

    print("\nğŸ§  [INFERENCE] ëª¨ë¸ë³„ ì¶”ë¡  ì‹œì‘")
    start_time = time.time()
    torch.cuda.cudart().cudaProfilerStart()

    for model_name in tqdm(model_list * iter, desc="ëª¨ë¸ ì¶”ë¡  ì§„í–‰ ì¤‘", unit="model"):
        # print(f"\nğŸ” {model_name} ì¶”ë¡  ì¤‘...")
        result = SessionManager.run_model(model_name)
        if isinstance(result, str) and "output:" in result:
            try:
                latency_str = result.split("output:")[1].split("ms")[0].strip()
                latency = float(latency_str)
                latency_stats[model_name].append(latency)
            except Exception as e:
                print(f"[WARN] latency íŒŒì‹± ì‹¤íŒ¨: {result}")
        else:
            print(f"[WARN] ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼ í˜•ì‹: {result}")

        # time.sleep(0.2)  # profiling ì‹œ í¸ì˜ ìœ„í•´ ê°„ê²© ì‚½ì…

    torch.cuda.cudart().cudaProfilerStop()
    end_time = time.time()
    total_time = end_time - start_time

    # ì¶”ë¡  íšŸìˆ˜ ë° throughput ê³„ì‚°
    total_inferences = len(model_list) * iter
    throughput = total_inferences / total_time

    print("\nâœ… ëª¨ë“  ì¶”ë¡  ì™„ë£Œ")
    print(f"â±ï¸ ì „ì²´ ì†Œìš” ì‹œê°„: {total_time:.3f}ì´ˆ")
    print(f"ğŸ“ˆ Inference Throughput: {throughput:.2f} inferences/sec")

    # ëª¨ë¸ë³„ latency í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ëª¨ë¸ë³„ Latency í†µê³„:")
    for model_name in model_list:
        latencies = latency_stats.get(model_name, [])
        if latencies:
            mean = np.mean(latencies)
            std = np.std(latencies)
            print(f"ğŸ”¹ {model_name}: í‰ê·  {mean:.3f} ms, í‘œì¤€í¸ì°¨ {std:.3f} ms, íšŸìˆ˜ {len(latencies)}íšŒ")
        else:
            print(f"âš ï¸ {model_name}: latency ë°ì´í„° ì—†ìŒ")