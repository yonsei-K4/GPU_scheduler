# run_parallel.py
import os, json, time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import torch
from tqdm import tqdm

from session import SessionManager   # ì§ˆë¬¸ì— ì£¼ì‹  session.py
import onnx
from onnx import NodeProto

ITER = 1000                     # â¬…ï¸ ëª¨ë¸ë‹¹ ë°˜ë³µ íšŸìˆ˜
MAX_WORKERS = 32               # GPU ë‹¹ ì ìœ  SM ìˆ˜ì— ë§ì¶° ì¡°ì •

def contains_loop(model_path: str) -> bool:
    g = onnx.load(model_path).graph
    return any(n.op_type == "Loop" for n in g.node)
###############################################################################
# 1. ëª¨ë¸ ëª©ë¡ ë¡œë“œ
###############################################################################
ROOT = os.path.dirname(__file__)
with open(os.path.join(ROOT, "models.json")) as f:
    model_list = json.load(f).get("models", [])

if not model_list:
    raise RuntimeError("models.json ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

###############################################################################
# 2. ì„¸ì…˜ ë¡œë“œ (ì„¸ì…˜ â‰’ ë…ë¦½ CUDA ìŠ¤íŠ¸ë¦¼)
###############################################################################
providers = dict()  # ëª¨ë¸ë³„ í”„ë¡œë°”ì´ë” ì„¤ì •}

for m in model_list:
    has_loop = contains_loop(os.path.join('/', "models", f"{m}.onnx"))

    if has_loop: 
        prov = ("CUDAExecutionProvider", {"device_id": 0})              # ê¸°ë³¸ ì„¤ì •
    else:
        prov = ("CUDAExecutionProvider", {"device_id": 0,
                                        "do_copy_in_default_stream": 0})
    providers[m] = prov

print("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...")
for m in model_list:
    if SessionManager.fetch_model(m, providers[m], batch_size=1) is None:
        raise RuntimeError(f"{m} ë¡œë“œ ì‹¤íŒ¨")
    else:
        print(f"âœ… {m} ë¡œë“œ ì™„ë£Œ")

# ì›Œë°ì—…
for m in model_list:
    SessionManager.run_model(m)

###############################################################################
# 3. ë³‘ë ¬ ì¶”ë¡  ì›Œì»¤
###############################################################################
def worker(model_name: str, iters: int):
    lat = []
    for k in range(iters):
        torch.cuda.nvtx.range_push(f"{model_name}_iter{k}")
        out = SessionManager.run_model(model_name)
        torch.cuda.nvtx.range_pop()

        ms = float(out.split("output:")[1].split("ms")[0].strip())
        lat.append(ms)
    return model_name, lat


torch.cuda.cudart().cudaProfilerStart()
t0 = time.time()


lat_dict = defaultdict(list)
with ThreadPoolExecutor(max_workers=len(model_list)) as pool:
    futures = [pool.submit(worker, m, ITER) for m in model_list]
    for f in tqdm(as_completed(futures), total=len(futures), unit="model"):
        name, l = f.result()
        lat_dict[name].extend(l)

torch.cuda.cudart().cudaProfilerStop()
elapsed = time.time() - t0
total_jobs = len(model_list) * ITER


print(f"\nâœ… {total_jobs} íšŒ ì¶”ë¡  ì™„ë£Œ â€“ {elapsed:.2f}s, "
      f"{total_jobs/elapsed:.1f} infer/s\n")
for m in model_list:
    l = lat_dict[m]
    print(f"ğŸ”¹ {m:20s}  í‰ê·  {np.mean(l):7.2f} ms  |  Ïƒ {np.std(l):6.2f} ms")